import requests
from bs4 import BeautifulSoup
import json
import time
from dotenv import load_dotenv
from supabase import create_client, Client
import os
from urllib.parse import urljoin, urlparse
import re
from datetime import datetime
import sys

# Add parent directory to path to import geocoding module
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from geocoding import geocode_location

# --- 1. Naƒçten√≠ p≈ô√≠stup≈Ø a inicializace klienta ---

# Explicitly load .env from backend directory (fix for local development)
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
env_path = os.path.join(backend_dir, '.env')
print(f"üîç Hled√°m .env soubor v: {env_path}")
if os.path.exists(env_path):
    print(f"‚úÖ .env soubor nalezen, naƒç√≠t√°m...")
    load_dotenv(dotenv_path=env_path)
else:
    print(f"‚ö†Ô∏è .env soubor nenalezen v {env_path}, zkou≈°√≠m v√Ωchoz√≠ um√≠stƒõn√≠...")
    load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
# Use SERVICE_KEY instead of ANON_KEY to bypass RLS policies
# The service role key has full access to all tables and ignores row-level security
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

# Debug output
print(f"   SUPABASE_URL: {'‚úÖ NAƒåTENO' if SUPABASE_URL else '‚ùå CHYB√ç'}")
print(f"   SUPABASE_SERVICE_KEY: {'‚úÖ NAƒåTENO' if SUPABASE_SERVICE_KEY else '‚ùå CHYB√ç'}")


def get_supabase_client():
    if not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
        print(
            "‚ö†Ô∏è VAROV√ÅN√ç: SUPABASE_URL nebo SUPABASE_SERVICE_KEY chyb√≠. Scrapov√°n√≠ bude fungovat, ale data se neulo≈æ√≠."
        )
        return None
    try:
        # Create client with service role key (bypasses RLS policies)
        client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
        print("‚úÖ √öspƒõ≈°nƒõ vytvo≈ôen klient Supabase (s pr√°vy service role).")
        return client
    except Exception as e:
        print(f"‚ùå Chyba p≈ôi inicializaci Supabase klienta: {e}")
        return None


supabase: Client = get_supabase_client()

# --- Pomocn√© funkce ---
def is_low_quality(job_data):
    """
    Checks if a job is low quality based on description length and blacklisted phrases.
    """
    description = job_data.get("description", "")
    if not description:
        return True
    
    # 1. Length check (User requested "500 words", but that's very strict. 
    # We'll start with 500 characters (~80 words) to filter out empty/one-line descriptions)
    if len(description) < 500:
        return True
        
    # 2. Blacklisted phrases
    blacklist = [
        "Prvn√≠ kontakt: e-mail p≈ôes odpovƒõdn√≠ formul√°≈ô",
        "Prvn√≠ kontakt: e-mail"
    ]
    
    desc_lower = description.lower()
    for phrase in blacklist:
        if phrase.lower() in desc_lower:
            return True
            
    return False

def now_iso():
    return datetime.utcnow().isoformat()


def norm_text(s):
    if not s:
        return ""
    return re.sub(r"\s+", " ", s).strip()


def extract_salary_range(stxt):
    """
    Centralizovan√° funkce pro extrakci platu z textu.
    Podporuje form√°ty: '35 000', '35.000', '35,000', '35-45 tis.' atd.
    """
    if not stxt:
        return None, None
    
    # Krok 1: Extrakce ƒç√≠seln√Ωch segment≈Ø vƒçetnƒõ oddƒõlovaƒç≈Ø (teƒçka, ƒç√°rka, mezera)
    nums = re.findall(r"\d[\d\s\.,]*", stxt)
    vals = []
    
    for x in nums:
        # Odstranƒõn√≠ mezer a teƒçek (ƒçasto tis√≠cov√© oddƒõlovaƒçe v ƒåR)
        cleaned = x.replace(" ", "").replace("\u00a0", "").replace(".", "")
        
        # O≈°et≈ôen√≠ ƒç√°rky - pokud je za n√≠ p≈ôesnƒõ 2 cifry na konci segmentu, jde o hal√©≈ôe
        if "," in cleaned:
            parts = cleaned.split(",")
            if len(parts) > 1 and len(parts[-1]) == 2:
                cleaned = parts[0] # Zahod√≠me hal√©≈ôe
            else:
                cleaned = cleaned.replace(",", "") # Jinak ƒç√°rka jako oddƒõlovaƒç tis√≠c≈Ø
        
        if cleaned:
            try:
                val = int(cleaned)
                # Filtrujeme nesmysln√© hodnoty (nap≈ô. '2024' z data nebo '1' z '1. patro')
                if val > 100:
                    vals.append(val)
            except ValueError:
                continue

    # Krok 2: Multiplik√°tor "tis√≠c"
    low_txt = stxt.lower()
    if "tis" in low_txt or "tis√≠c" in low_txt:
        # Pokud jsou hodnoty podez≈ôele mal√© (nap≈ô. 35 m√≠sto 35000), vyn√°sob√≠me je
        vals = [v * 1000 if v < 1000 else v for v in vals]

    # Krok 3: Urƒçen√≠ From/To
    salary_from = None
    salary_to = None
    
    if len(vals) == 1:
        salary_from = vals[0]
    elif len(vals) >= 2:
        # Se≈ôad√≠me, aby from < to (pokud by byly v opaƒçn√©m po≈ôad√≠ v textu)
        salary_from = min(vals[0], vals[1])
        salary_to = max(vals[0], vals[1])
        
    return salary_from, salary_to


# --- Ulo≈æen√≠ do Supabase ---
def save_job_to_supabase(job_data):
    if not supabase:
        print("Chyba: Supabase klient nen√≠ inicializov√°n, data nebudou ulo≈æena.")
        return False

    try:
        response = (
            supabase.table("jobs").select("url").eq("url", job_data["url"]).execute()
        )
        if response.data:
            print(f"    --> Nab√≠dka s URL {job_data['url']} ji≈æ existuje, p≈ôeskoƒçeno.")
            return True
    except Exception as e:
        print(f"Chyba p≈ôi kontrole duplicity: {e}")

    parsed_url = urlparse(job_data["url"])
    job_data["source"] = parsed_url.netloc.replace("www.", "")
    job_data.setdefault("scraped_at", now_iso())
    job_data.setdefault("legality_status", "legal") # Default to legal for scraped jobs
    
    # DETECT AND ASSIGN COUNTRY CODE based on domain
    domain = parsed_url.netloc.lower()
    if '.cz' in domain:
        job_data["country_code"] = "cs"
    elif '.sk' in domain:
        job_data["country_code"] = "sk"
    elif '.pl' in domain:
        job_data["country_code"] = "pl"
    elif '.de' in domain:
        job_data["country_code"] = "de"
    else:
        # Default to Czech if domain is unknown
        job_data["country_code"] = "cs"
    
    print(f"    üåç Country code: {job_data['country_code']} (detected from {domain})")
    
    # GEOCODE LOCATION: Convert location string to lat/lon
    if "location" in job_data and job_data["location"]:
        location_str = job_data["location"]
        print(f"    üåç Geocodov√°n√≠ lokality: {location_str}")
        
        geo_result = geocode_location(location_str)
        if geo_result:
            job_data["lat"] = geo_result["lat"]
            job_data["lng"] = geo_result["lon"]
            print(f"       ‚úÖ Nalezeno: ({geo_result['lat']:.4f}, {geo_result['lon']:.4f}) [{geo_result['source']}]")
        else:
            print(f"       ‚ö†Ô∏è Geolokace selhala, ulo≈æ sem bez sou≈ôadnic")
            job_data["lat"] = None
            job_data["lng"] = None

    try:
        response = supabase.table("jobs").insert(job_data).execute()
        if response.data:
            print(f"    --> Data pro '{job_data.get('title')}' √∫spƒõ≈°nƒõ ulo≈æena.")
            return True
        else:
            print(f"    ‚ùå Chyba p≈ôi ukl√°d√°n√≠ dat: {job_data.get('title')}")
            return False
    except Exception as e:
        print(f"    ‚ùå Do≈°lo k neoƒçek√°van√© chybƒõ p≈ôi ukl√°d√°n√≠: {e}")
        return False


# --- Stahov√°n√≠ str√°nky ---
def scrape_page(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        resp = requests.get(url, headers=headers, timeout=12)
        resp.raise_for_status()
        return BeautifulSoup(resp.content, "html.parser")
    except Exception as e:
        print(f"‚ùå Chyba p≈ôi stahov√°n√≠ {url}: {e}")
        return None


# --- Filtrov√°n√≠ footeru ---
def filter_out_junk(text):
    """Odstran√≠ navigaci, patiƒçky a obecn√Ω balast z popis≈Ø pozic."""
    if not text:
        return ""
    
    # Rozs√°hl√Ω seznam "junk" token≈Ø, kter√© se ƒçasto objevuj√≠ v navigaci nebo patiƒçk√°ch
    junk_tokens = [
        "nab√≠dky pr√°ce", "vytvo≈ôit si ≈æivotopis", "jobs.cz", "prace.cz", "atmoskop",
        "profesia.sk", "profesia.cz", "pr√°ca za rohom", "pr√°ce za rohem", "nelisa.com",
        "arnold", "teamio", "seduo.cz", "seduo.sk", "platy.cz", "platy.sk", "paylab.com",
        "mojposao", "historie odpovƒõd√≠", "ulo≈æen√© nab√≠dky", "upozornƒõn√≠ na nab√≠dky",
        "hled√°m zamƒõstnance", "vlo≈æit brig√°du", "cen√≠k inzerce", "napi≈°te n√°m",
        "pro m√©dia", "z√°sady ochrany soukrom√≠", "podm√≠nky pou≈æ√≠v√°n√≠", "nastaven√≠ cookies",
        "reklama na port√°lech", "transparentnost", "nahl√°sit nez√°konn√Ω obsah",
        "vzdƒõl√°vac√≠ kurzy", "st≈ôedo≈°kolsk√© nebo odborn√©", "typ pracovn√≠ho pomƒõru",
        "kontaktn√≠ √∫daje", "zadavatel", "ƒçast√© pracovn√≠ cesty", "foto v medailonku",
        "the pulse of beauty", "nadn√°rodn√≠ struktury", "vlastn√≠ organizace",
        "vyhrazen√Ω ƒças na inovace", "kafet√©rie", "p≈ô√≠spƒõvek na vzdƒõl√°n√≠",
        "stravenky/p≈ô√≠spƒõvek na stravov√°n√≠", "zdravotn√≠ volno/sickdays",
        "mo≈ænost obƒçasn√© pr√°ce z domova", "obƒçerstven√≠ na pracovi≈°ti",
        "p≈ô√≠spƒõvek na sport/kulturu", "firemn√≠ akce", "bonusy/pr√©mie",
        "flexibiln√≠ zaƒç√°tek/konec pracovn√≠ doby", "notebook", "sleva na firemn√≠ v√Ωrobky",
        "nab√≠dky pr√°ce", "brig√°dy", "inspirace", "zamƒõstnavatel√©", "skvƒõl√Ω ≈æivotopis",
        "m≈Ø≈æete si ho ulo≈æit", "vytisknout nebo poslat do svƒõta"
    ]
    
    lines = text.split("\n")
    filtered_lines = []
    
    for line in lines:
        stripped = line.strip()
        if not stripped:
            filtered_lines.append("")
            continue
            
        low = stripped.lower()
        
        # Pokud je ≈ô√°dek p≈ô√≠li≈° kr√°tk√Ω (navigaƒçn√≠ odkaz) a obsahuje junk token
        if len(stripped) < 100:
            if any(tok in low for tok in junk_tokens):
                continue
        
        # Specifick√© pro Jobs.cz navigaci (ƒçasto dlouh√© seznamy s kr√°tk√Ωmi ≈ô√°dky)
        if any(tok == low for tok in junk_tokens):
            continue
            
        filtered_lines.append(stripped)
    
    # Zpƒõtn√© spojen√≠ a vyƒçi≈°tƒõn√≠ pr√°zdn√Ωch ≈ô√°dk≈Ø na zaƒç√°tku/konci
    result = "\n".join(filtered_lines).strip()
    
    # Odstranƒõn√≠ v√≠cen√°sobn√Ωch pr√°zdn√Ωch ≈ô√°dk≈Ø
    result = re.sub(r'\n{3,}', '\n\n', result)
    
    return result if result else "Popis nen√≠ dostupn√Ω"


# Ponech√°v√°me filter_jenprace_footer pro zpƒõtnou kompatibilitu, 
# ale internƒõ vol√° filter_out_junk
def filter_jenprace_footer(text):
    return filter_out_junk(text)


# --- Jobs.cz ---
def scrape_jobs_cz(soup):
    jobs_saved = 0
    job_cards = soup.find_all("article")
    for card in job_cards:
        header = card.find("header")
        footer = card.find("footer")
        if not header or not footer:
            continue

        nazev_element = header.find("h2", class_="SearchResultCard__title")
        if not (nazev_element and nazev_element.a):
            continue
        title = norm_text(nazev_element.a.text)
        odkaz = urljoin("https://www.jobs.cz", nazev_element.a["href"])

        # Firma a lokalita
        company = "Nezn√°m√° spoleƒçnost"
        comp_el = footer.find("li", class_="SearchResultCard__footerItem")
        if comp_el:
            span = comp_el.find("span", {"translate": "no"})
            if span:
                company = norm_text(span.text)
        location = "Nezn√°m√° lokalita"
        loc_el = footer.find(
            "li",
            class_="SearchResultCard__footerItem",
            attrs={"data-test": "serp-locality"},
        )
        if loc_el:
            location = norm_text(loc_el.text)

        print(f"    Stahuji detail pro: {title}")
        detail_soup = scrape_page(odkaz)
        if not detail_soup:
            print(f"    --> Detail str√°nka nedostupn√°, p≈ôeskoƒçeno.")
            continue
        description = "Popis nenalezen"
        benefits = []
        salary_from = None
        salary_to = None
        contract_type = "Nespecifikov√°no"

        if detail_soup:
            try:
                # Popis - Jobs.cz uses RichContent class for job descriptions
                # This is the main container with all the formatted job description content
                main_content = detail_soup.find("div", class_="RichContent")
                
                # Fallback to older selectors if RichContent not found
                if not main_content:
                    main_content = detail_soup.find("div", class_="JobDescriptionSection") or detail_soup.find("div", class_="JobDescription")
                
                parts = []
                if main_content:
                    # Extract all meaningful content: paragraphs, headings, and list items
                    for elem in main_content.find_all(['p', 'li', 'h2', 'h3', 'h4', 'ul', 'ol']):
                        # Skip ul/ol containers themselves, we only want their li children
                        if elem.name in ['ul', 'ol']:
                            continue
                            
                        txt = norm_text(elem.get_text())
                        if not txt:
                            continue
                        
                        # Format based on element type
                        if elem.name == 'li':
                            parts.append(f"- {txt}")
                        elif elem.name in ['h2', 'h3', 'h4']:
                            parts.append(f"\n### {txt}")
                        else:  # p tags
                            parts.append(txt)

                if parts:
                    description = filter_out_junk("\n\n".join(parts))


                # Benefity
                btitle = detail_soup.find(
                    "p",
                    class_="typography-body-medium-text-regular JobDescriptionBenefits__title mb-600 text-secondary",
                )
                if btitle:
                    bdivs = btitle.find_next_siblings("div", class_="IconWithText")
                    benefits = [
                        norm_text(d.get_text())
                        for d in bdivs
                        if norm_text(d.get_text())
                    ]

                # Plat
                sal_div = detail_soup.find("div", {"data-test": "jd-salary"})
                if sal_div:
                    p = sal_div.find("p")
                    if p:
                        stxt = p.get_text(" ", strip=True)
                        salary_from, salary_to = extract_salary_range(stxt)

                # Typ smluvn√≠ho vztahu
                info_items = detail_soup.find_all("div", {"data-test": "jd-info-item"})
                for item in info_items:
                    label = item.find("span", class_="accessibility-hidden")
                    val = item.find("p")
                    if label and "Typ smluvn√≠ho vztahu" in label.get_text():
                        if val:
                            contract_type = norm_text(val.get_text())
            except Exception as e:
                print(f"    ‚ùå Chyba detailu {odkaz}: {e}")

        if not benefits:
            benefits = ["Benefity nespecifikov√°ny"]

        job_data = {
            "title": title,
            "url": odkaz,
            "company": company,
            "location": location,
            "description": description,
            "benefits": benefits,
            "contract_type": contract_type,
            "salary_from": salary_from,
            "salary_to": salary_to,
        }

        # Quality Check
        if is_low_quality(job_data):
            print(f"    ‚ö†Ô∏è N√≠zk√° kvalita, p≈ôeskakuji: {title}")
            continue

        if save_job_to_supabase(job_data):
            jobs_saved += 1
        time.sleep(0.3)
    return jobs_saved


# --- Prace.cz ---
def scrape_prace_cz(soup):
    jobs_saved = 0
    job_links = soup.find_all("a", attrs={"data-jd": True})
    for link in job_links:
        title = norm_text(link.get_text())
        odkaz = urljoin("https://www.prace.cz", link["href"])
        print(f"    Stahuji detail pro: {title}")
        detail_soup = scrape_page(odkaz)
        if not detail_soup:
            print(f"    --> Detail str√°nka nedostupn√°, p≈ôeskoƒçeno.")
            continue

        company = "Nezn√°m√° spoleƒçnost"
        location = "Nezn√°m√° lokalita"
        employment_type = "Nespecifikov√°no"
        contract_type = "Nespecifikov√°no"
        benefits = []
        description = "Popis nenalezen"
        salary_from = None
        salary_to = None

        if detail_soup:
            try:
                # Firma
                comp_el = detail_soup.find("dd", class_="advert__list--company-name")
                if comp_el:
                    strong = comp_el.find("strong")
                    if strong:
                        company = norm_text(strong.get_text())

                # Lokalita
                loc_el = detail_soup.find("dd", class_="advert__list--location")
                if loc_el:
                    span = loc_el.find("span", class_="data")
                    if span:
                        location = norm_text(span.get_text())

                # Druh √∫vazku
                empl_el = detail_soup.find("dd", class_="advert__list--employment-type")
                if empl_el:
                    div = empl_el.find("div", class_="data")
                    if div:
                        employment_type = norm_text(div.get_text())

                # Smluvn√≠ vztah
                contract_el = detail_soup.find(
                    "dd", class_="advert__list--contract-type"
                )
                if contract_el:
                    div = contract_el.find("div", class_="data")
                    if div:
                        contract_type = norm_text(div.get_text())

                # Benefity
                benefit_el = detail_soup.find("dd", class_="advert__list--benefit")
                if benefit_el:
                    span = benefit_el.find("span", class_="data")
                    if span:
                        benefits = [
                            b.strip() for b in span.get_text().split(",") if b.strip()
                        ]

                # Plat
                sal_h3 = detail_soup.find("h3", class_="advert__salary")
                if sal_h3:
                    stxt = sal_h3.get_text(" ", strip=True)
                    salary_from, salary_to = extract_salary_range(stxt)

                # Popis
                desc_el = detail_soup.find("div", class_="advert__richtext")
                if desc_el:
                    parts = []
                    for child in desc_el.find_all(["p", "li", "h2", "h3"], recursive=True):
                        txt = norm_text(child.get_text())
                        if not txt:
                            continue
                        if child.name == "li":
                            parts.append(f"- {txt}")
                        elif child.name in ["h2", "h3"]:
                            parts.append(f"### {txt}")
                        else:
                            parts.append(txt)
                    
                    if parts:
                        description = filter_out_junk("\n\n".join(parts))
            except Exception as e:
                print(f"    ‚ùå Chyba detailu {odkaz}: {e}")

        if not benefits:
            benefits = ["Benefity nespecifikov√°ny"]

        job_data = {
            "title": title,
            "url": odkaz,
            "company": company,
            "location": location,
            "description": description,
            "benefits": benefits,
            "contract_type": contract_type,
            "work_type": employment_type,
            "salary_from": salary_from,
            "salary_to": salary_to,
        }

        if save_job_to_supabase(job_data):
            jobs_saved += 1
        time.sleep(0.3)
    return jobs_saved


# --- Jenprace.cz --- (ponech√°no z p≈ôedchoz√≠ verze s benefity)
def scrape_jenprace_cz(soup):
    jobs_saved = 0
    job_cards = soup.find_all("a", class_="container-link")
    for card in job_cards:
        odkaz = card.get("href")
        if not odkaz or odkaz == "#":
            continue
        title_element = card.find("span", class_="offer-link")
        title = norm_text(title_element.text if title_element else "Nezn√°m√Ω n√°zev")
        detail_url = urljoin("https://www.jenprace.cz", odkaz)
        detail_soup = scrape_page(detail_url)
        if not detail_soup:
            print(f"    --> Detail str√°nka nedostupn√°, p≈ôeskoƒçeno.")
            continue

        # Oprava: hledej lokaci v detail_soup m√≠sto soup + pou≈æij fix_duplicated_city
        location = None
        if detail_soup:
            location_tag = detail_soup.select_one(
                "div.value[data-cy='locality-detail-value']"
            )
            if location_tag:
                location_text = location_tag.get_text(strip=True)
                location = fix_duplicated_city(location_text.split("(")[0].strip())
            else:
                location_tag = detail_soup.select_one(
                    "span.locality[data-cy='offer-locality']"
                )
                if location_tag:
                    location_text = location_tag.get_text(strip=True)
                    location = fix_duplicated_city(location_text.split("(")[0].strip())

        company = "Nezn√°m√° spoleƒçnost"
        if detail_soup:
            comp = detail_soup.find("div", {"data-cy": "company-value"})
            if comp:
                a = comp.find("a")
                company = norm_text(a.get_text() if a else comp.get_text())

        description = "Popis nenalezen"
        contract_type = "Nespecifikov√°no"
        education_level = "Nespecifikov√°no"
        benefits = []
        salary_from = None
        salary_to = None

        if detail_soup:
            # Popis
            desc = detail_soup.find("div", class_="offer-content")
            if desc:
                parts = []
                for child in desc.find_all(["p", "li"], recursive=True):
                    txt = norm_text(child.get_text())
                    if not txt:
                        continue
                    if child.name == "li":
                        parts.append(f"- {txt}")
                    else:
                        parts.append(txt)
                
                if parts:
                    description = filter_out_junk("\n\n".join(parts))
            # Benefity
            blist = detail_soup.find("ul", {"data-cy": "offer-benefit-list"})
            if blist:
                items = blist.find_all("li", {"data-cy": "offer-benefit-item"})
                benefits = [
                    norm_text(i.get_text()) for i in items if norm_text(i.get_text())
                ]
            if not benefits:
                benefits = ["Benefity nespecifikov√°ny"]
            # Contract
            ct = detail_soup.find("div", {"data-cy": "relation-value"})
            if ct:
                contract_type = norm_text(ct.get_text())
            # Education
            edu = detail_soup.find("div", {"data-cy": "education-value"})
            if edu:
                education_level = norm_text(edu.get_text())

            # Plat
            sal_div = detail_soup.find("div", class_="label-reward")
            if sal_div:
                stxt = sal_div.get_text(" ", strip=True)
                salary_from, salary_to = extract_salary_range(stxt)

        job_data = {
            "title": title,
            "url": detail_url,
            "company": company,
            "location": location,
            "description": description,
            "benefits": benefits,
            "contract_type": contract_type,
            "education_level": education_level,
            "salary_from": salary_from,
            "salary_to": salary_to,
        }
        if save_job_to_supabase(job_data):
            jobs_saved += 1
        time.sleep(0.3)
    return jobs_saved


# --- Hlavn√≠ funkce ---
def scrape_website(site_name, base_url, max_pages=10):
    total_saved = 0
    scrapers = {
        "jobs.cz": scrape_jobs_cz,
        "prace.cz": scrape_prace_cz,
        "jenprace.cz": scrape_jenprace_cz,
    }
    scraper_func = scrapers.get(site_name)
    if not scraper_func:
        print(f"‚ùå Nepodporovan√Ω web: {site_name}")
        return 0
    for page_num in range(1, max_pages + 1):
        url = (
            f"{base_url}&page={page_num}"
            if site_name == "jobs.cz"
            else f"{base_url}?page={page_num}"
        )
        soup = scrape_page(url)
        if not soup:
            continue
        jobs = scraper_func(soup)
        total_saved += jobs
        if jobs == 0:
            break
        time.sleep(2)
    print(f"Scrapov√°n√≠ {site_name} dokonƒçeno. Celkem {total_saved}.")
    return total_saved


def fix_duplicated_city(text):
    """Oprav√≠ duplicitn√≠ mƒõsto (OlomoucOlomouc -> Olomouc) a odstran√≠ 'zobrazit na mapƒõ'"""
    # Odstranƒõn√≠ textu "zobrazit na mapƒõ" a podobn√Ωch
    text = text.replace("zobrazit na mapƒõ", "").replace("Zobrazit na mapƒõ", "").strip()

    # Oprava duplicitn√≠ho mƒõsta
    if len(text) > 1 and len(text) % 2 == 0:
        half = len(text) // 2
        if text[:half] == text[half:]:
            return text[:half]
    return text


def run_all_scrapers():
    if not supabase:
        print("‚ùå Supabase nen√≠ dostupn√©. Scrapov√°n√≠ zru≈°eno.")
        return 0

    websites = [
        {
            "name": "jobs.cz",
            "base_url": "https://www.jobs.cz/prace/?language-skill=cs",
            "max_pages": 15,
        },
        {
            "name": "prace.cz",
            "base_url": "https://www.prace.cz/nabidky",
            "max_pages": 15,
        },
        {
            "name": "jenprace.cz",
            "base_url": "https://www.jenprace.cz/nabidky",
            "max_pages": 15,
        },
    ]

    grand_total = 0
    print(f"üöÄ Spou≈°t√≠m hromadn√© scrapov√°n√≠: {now_iso()}")
    for site in websites:
        try:
            grand_total += scrape_website(
                site["name"], site["base_url"], site["max_pages"]
            )
        except Exception as e:
            print(f"‚ùå Chyba p≈ôi scrapov√°n√≠ {site['name']}: {e}")

    print(f"‚úÖ Scrapov√°n√≠ dokonƒçeno. Celkem ulo≈æeno {grand_total} nab√≠dek.")
    return grand_total


if __name__ == "__main__":
    run_all_scrapers()
