import requests
from bs4 import BeautifulSoup
import json
import time
from dotenv import load_dotenv
from supabase import create_client, Client
import os
from urllib.parse import urljoin, urlparse
import re
from datetime import datetime
import sys
from langdetect import detect, LangDetectException

# Add parent directory to path to import geocoding module
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from geocoding import geocode_location

# --- 1. Naƒçten√≠ p≈ô√≠stup≈Ø a inicializace klienta ---

# Explicitly load .env from backend directory (fix for local development)
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
env_path = os.path.join(backend_dir, '.env')
print(f"üîç Hled√°m .env soubor v: {env_path}")
if os.path.exists(env_path):
    print(f"‚úÖ .env soubor nalezen, naƒç√≠t√°m...")
    load_dotenv(dotenv_path=env_path)
else:
    print(f"‚ö†Ô∏è .env soubor nenalezen v {env_path}, zkou≈°√≠m v√Ωchoz√≠ um√≠stƒõn√≠...")
    load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
# Use SERVICE_KEY instead of ANON_KEY to bypass RLS policies
# The service role key has full access to all tables and ignores row-level security
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

# Debug output
print(f"   SUPABASE_URL: {'‚úÖ NAƒåTENO' if SUPABASE_URL else '‚ùå CHYB√ç'}")
print(f"   SUPABASE_SERVICE_KEY: {'‚úÖ NAƒåTENO' if SUPABASE_SERVICE_KEY else '‚ùå CHYB√ç'}")


def get_supabase_client():
    if not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
        print(
            "‚ö†Ô∏è VAROV√ÅN√ç: SUPABASE_URL nebo SUPABASE_SERVICE_KEY chyb√≠. Scrapov√°n√≠ bude fungovat, ale data se neulo≈æ√≠."
        )
        return None
    try:
        # Create client with service role key (bypasses RLS policies)
        client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
        print("‚úÖ √öspƒõ≈°nƒõ vytvo≈ôen klient Supabase (s pr√°vy service role).")
        return client
    except Exception as e:
        print(f"‚ùå Chyba p≈ôi inicializaci Supabase klienta: {e}")
        return None


supabase: Client = get_supabase_client()

# --- Pomocn√© funkce ---
def is_low_quality(job_data):
    """
    Checks if a job is low quality based on description length and blacklisted phrases.
    """
    description = job_data.get("description", "")
    if not description:
        return True
    
    # 1. Length check (User requested "500 words", but that's very strict. 
    # We'll start with 500 characters (~80 words) to filter out empty/one-line descriptions)
    if len(description) < 500:
        return True
        
    # 2. Blacklisted phrases
    blacklist = [
        "Prvn√≠ kontakt: e-mail p≈ôes odpovƒõdn√≠ formul√°≈ô",
        "Prvn√≠ kontakt: e-mail",
        "kontakt telefonem",
        "konec inzer√°tu",
        "konec inzeratu",
        "telefonem"
    ]
    
    desc_lower = description.lower()
    for phrase in blacklist:
        if phrase.lower() in desc_lower:
            return True
            
    return False


def is_external_listing(detail_soup, allowed_domains):
    """
    Heuristic detection of external (offsite) job listings.
    Returns True if canonical/og URL or apply links point outside allowed domains.
    """
    try:
        # Canonical / OG URL checks
        canonical = detail_soup.find("link", rel="canonical")
        if canonical and canonical.get("href"):
            canon_domain = urlparse(canonical["href"]).netloc.lower()
            if canon_domain and not any(d in canon_domain for d in allowed_domains):
                return True

        og_url = detail_soup.find("meta", property="og:url")
        if og_url and og_url.get("content"):
            og_domain = urlparse(og_url["content"]).netloc.lower()
            if og_domain and not any(d in og_domain for d in allowed_domains):
                return True

        # Apply / redirect links to external domains
        apply_keywords = ["apply", "odpoved", "odpovƒõ", "career", "job", "position", "recruitee",
                          "greenhouse", "workday", "icims", "successfactors", "taleo",
                          "smartrecruiters", "lever", "personio", "teamio", "nelisa"]
        for a in detail_soup.find_all("a", href=True):
            href = a["href"]
            if href.startswith("http"):
                domain = urlparse(href).netloc.lower()
                if domain and not any(d in domain for d in allowed_domains):
                    text = (a.get_text() or "").lower()
                    if any(k in href.lower() for k in apply_keywords) or any(k in text for k in apply_keywords):
                        return True
    except Exception:
        pass
    return False

def now_iso():
    return datetime.utcnow().isoformat()


def norm_text(s):
    if not s:
        return ""
    return re.sub(r"\s+", " ", s).strip()


def detect_language_code(text: str):
    if not text:
        return None
    cleaned = norm_text(text)
    if len(cleaned) < 80:
        return None
    try:
        return detect(cleaned)
    except LangDetectException:
        return None
    except Exception:
        return None


def detect_salary_timeframe_cz(text):
    if not text:
        return None
    low = text.lower()
    if any(tok in low for tok in ["kƒç/h", "kc/h", "/hod", "hodin", "hod."]):
        return "hour"
    if any(tok in low for tok in ["/den", "dennƒõ", "denne"]):
        return "day"
    if any(tok in low for tok in ["/t√Ωd", "/tyd", "t√Ωden", "tyden"]):
        return "week"
    if any(tok in low for tok in ["/mƒõs", "/mes", "mƒõs√≠c", "mesiac", "mƒõs√≠ƒçnƒõ", "mesicne"]):
        return "month"
    if any(tok in low for tok in ["/rok", "roƒç", "roc", "p.a."]):
        return "year"
    return None


def detect_working_time_cz(text):
    if not text:
        return None
    low = text.lower()
    if "pln√Ω" in low or "full" in low:
        return "full_time"
    if any(tok in low for tok in ["zkr√°cen", "zkracen", "ƒç√°steƒç", "castec", "part"]):
        return "part_time"
    if any(tok in low for tok in ["brig√°d", "brigad", "dpp", "dpƒç", "dpc"]):
        return "temporary"
    if any(tok in low for tok in ["≈æivnost", "zivnost", "iƒço", "ico", "osvƒç", "osvc", "freelance"]):
        return "contract"
    if any(tok in low for tok in ["st√°≈æ", "staz", "intern", "trainee"]):
        return "internship"
    if any(tok in low for tok in ["uƒç≈à", "ucn", "apprentice"]):
        return "apprenticeship"
    return None


def detect_work_model_cz(location, title, description):
    text = " ".join([location or "", title or "", description or ""]).lower()
    if any(tok in text for tok in ["hybrid", "ƒç√°steƒçnƒõ z domova", "castecne z domova", "kombinovan"]):
        return "hybrid"
    if any(tok in text for tok in ["home office", "homeoffice", "pr√°ce z domova", "praca z domu", "remote", "na d√°lku", "na dalku", "d√°lkov"]):
        return "remote"
    if location:
        return "onsite"
    return None


def detect_job_level_cz(title):
    if not title:
        return None
    low = title.lower()
    if any(tok in low for tok in ["st√°≈æ", "staz", "intern", "trainee"]):
        return "internship"
    if any(tok in low for tok in ["uƒç≈à", "ucn", "apprentice"]):
        return "apprenticeship"
    if any(tok in low for tok in ["student", "diplom", "diserta", "phd", "doktorand"]):
        return "student"
    if any(tok in low for tok in ["junior", "jr.", "jr ", "absolvent", "bez praxe"]):
        return "junior"
    if any(tok in low for tok in ["mid", "medior"]):
        return "mid"
    if any(tok in low for tok in ["senior", "sr."]):
        return "senior"
    if any(tok in low for tok in ["lead", "head", "director", "vedouc", "veduci"]):
        return "lead"
    if any(tok in low for tok in ["chief", "ceo", "cto", "cfo"]):
        return "executive"
    if any(tok in low for tok in ["≈æivnost", "zivnost", "freelance", "self-employed"]):
        return "self_employed"
    return None


def extract_required_skills_cz(detail_soup, description):
    """
    Extract required skills/requirements from CZ detail pages or description.
    Returns list of strings.
    """
    skills = []
    seen = set()

    def add_line(txt):
        if not txt:
            return
        if txt in seen:
            return
        if len(txt) > 200:
            return
        seen.add(txt)
        skills.append(txt)

    def collect_from_heading(head):
        lines = []
        for sib in head.next_siblings:
            name = getattr(sib, "name", None)
            if name in ["h2", "h3", "h4"]:
                break
            if name in ["ul", "ol"]:
                for li in sib.find_all("li"):
                    txt = norm_text(li.get_text())
                    if txt:
                        lines.append(txt)
            elif name in ["p", "div", "span"]:
                txt = norm_text(sib.get_text())
                if txt:
                    lines.append(txt)
        return lines

    if detail_soup:
        keywords = [
            "po≈æadujeme", "pozadujeme", "po≈æadavky", "pozadavky",
            "po≈æadavky na", "pozadavky na", "co od v√°s oƒçek√°v√°me", "co od vas ocekavame",
            "znalosti", "dovednosti", "schopnosti", "kvalifikace",
            "profil", "requirements", "your profile", "skills"
        ]
        for head in detail_soup.find_all(["h2", "h3", "h4", "strong", "b"]):
            htxt = norm_text(head.get_text()).lower()
            if not htxt:
                continue
            if any(k in htxt for k in keywords):
                lines = collect_from_heading(head)
                for line in lines:
                    add_line(line)
                if skills:
                    break

    # Fallback: parse description bullets after keyword line
    if not skills and description:
        lines = [l.strip() for l in description.split("\n") if l.strip()]
        hit = False
        for line in lines:
            low = line.lower()
            if any(k in low for k in ["po≈æadujeme", "pozadujeme", "po≈æadavky", "pozadavky", "requirements", "your profile", "skills"]):
                hit = True
                continue
            if hit:
                if line.startswith("- "):
                    add_line(line[2:].strip())
                elif line.startswith("### "):
                    break
        # If still empty, take bullet lines that look like skills
        if not skills:
            for line in lines:
                if line.startswith("- "):
                    add_line(line[2:].strip())

    return skills


def normalize_required_skills_cz(skills):
    """
    Normalize and clean skill list for CZ market.
    Returns tuple (hard_skills, soft_skills).
    """
    if not skills:
        return [], []

    canonical_map = {
        "ms excel": "Excel",
        "microsoft excel": "Excel",
        "excel": "Excel",
        "ms word": "Word",
        "microsoft word": "Word",
        "word": "Word",
        "powerpoint": "PowerPoint",
        "ms powerpoint": "PowerPoint",
        "microsoft powerpoint": "PowerPoint",
        "ms office": "MS Office",
        "microsoft office": "MS Office",
        "office": "MS Office",
        "sql": "SQL",
        "mysql": "MySQL",
        "postgres": "PostgreSQL",
        "postgresql": "PostgreSQL",
        "mssql": "MS SQL",
        "t-sql": "MS SQL",
        "python": "Python",
        "java": "Java",
        "javascript": "JavaScript",
        "js": "JavaScript",
        "typescript": "TypeScript",
        "ts": "TypeScript",
        "c++": "C++",
        "c#": "C#",
        "c": "C",
        "php": "PHP",
        "html": "HTML",
        "css": "CSS",
        "html/css": "HTML, CSS",
        "react": "React",
        "react.js": "React",
        "angular": "Angular",
        "vue": "Vue.js",
        "node": "Node.js",
        "node.js": "Node.js",
        "docker": "Docker",
        "kubernetes": "Kubernetes",
        "k8s": "Kubernetes",
        "aws": "AWS",
        "azure": "Azure",
        "gcp": "GCP",
        "git": "Git",
        "linux": "Linux",
        "windows": "Windows",
        "jira": "Jira",
        "sap": "SAP",
        "erp": "ERP",
        "crm": "CRM",
        "autocad": "AutoCAD",
        "solidworks": "SolidWorks",
        "matlab": "MATLAB",
        "excel (pokroƒçil√Ω)": "Excel (pokroƒçil√Ω)",
        "angliƒçtina": "Angliƒçtina",
        "nƒõmƒçina": "Nƒõmƒçina",
        "nemcina": "Nƒõmƒçina",
        "ru≈°tina": "Ru≈°tina",
        "rustina": "Ru≈°tina",
        "ƒçe≈°tina": "ƒåe≈°tina",
        "cestina": "ƒåe≈°tina",
        "sloven≈°tina": "Sloven≈°tina",
        "slovencina": "Sloven≈°tina",
    }

    soft_keywords = [
        "komunikativ", "samostatn", "spolehliv", "peƒçliv", "pecliv",
        "zodpovƒõdn", "zodpovedn", "flexibil", "proaktiv", "t√Ωmov", "tymov",
        "odolnost", "asertiv", "organizac", "time management", "leadership",
        "prezentac", "motivac", "adaptabil", "kreativ", "loajal",
        "preciz", "empati", "multitasking"
    ]

    language_aliases = {
        "en": ["aj", "angliƒçtina", "anglictina", "english", "en"],
        "de": ["nj", "nƒõmƒçina", "nemcina", "nemƒçina", "german", "de"],
        "fr": ["francouz≈°tina", "francouzstina", "fr", "french"],
        "es": ["≈°panƒõl≈°tina", "spanelstina", "es", "spanish"],
        "it": ["ital≈°tina", "italstina", "it", "italian"],
        "ru": ["ru≈°tina", "rustina", "ru", "russian"],
        "cs": ["ƒçe≈°tina", "cestina", "cz", "czech"],
        "sk": ["sloven≈°tina", "slovencina", "sk", "slovak"],
        "pl": ["pol≈°tina", "polstina", "pl", "polish"],
    }
    language_names = {
        "en": "Angliƒçtina",
        "de": "Nƒõmƒçina",
        "fr": "Francouz≈°tina",
        "es": "≈†panƒõl≈°tina",
        "it": "Ital≈°tina",
        "ru": "Ru≈°tina",
        "cs": "ƒåe≈°tina",
        "sk": "Sloven≈°tina",
        "pl": "Pol≈°tina",
    }

    hard_skills = []
    soft_skills = []
    seen_hard = set()
    seen_soft = set()

    def clean_token(t):
        t = norm_text(t)
        t = re.sub(r"^[\-\‚Ä¢\*\d\)\.\s]+", "", t)
        t = t.strip()
        return t

    def is_noise(t):
        low = t.lower()
        if len(low) < 2:
            return True
        if len(low) > 120:
            return True
        noise_tokens = [
            "praxe", "zku≈°enost", "zku≈°enosti", "vzdelani", "vzdƒõl√°n√≠",
            "min.", "minim", "nutn√°", "nutne", "po≈æadujeme", "po≈æadavky",
            "pozadujeme", "pozadavky", "v√Ωhodou", "vyhodou"
        ]
        if any(tok in low for tok in noise_tokens):
            return True
        return False

    def parse_language_skill(t):
        low = t.lower()
        # Detect language
        lang_code = None
        for code, aliases in language_aliases.items():
            for alias in aliases:
                if re.search(rf"\b{re.escape(alias)}\b", low):
                    lang_code = code
                    break
            if lang_code:
                break
        if not lang_code:
            return None

        # Detect level (CEFR or descriptive)
        level = None
        levels = re.findall(r"\b([abc][12])\b", low)
        if levels:
            level = "/".join([l.upper() for l in levels])
        else:
            if re.search(r"rodil|native", low):
                level = "Native"
            elif re.search(r"plynul|fluent", low):
                level = "C1"
            elif re.search(r"pokroƒçi|pokrocil", low):
                if re.search(r"st≈ôedn|stredn", low):
                    level = "B2"
                elif re.search(r"m√≠rn|mirn", low):
                    level = "B1"
                else:
                    level = "C1"
            elif re.search(r"aktivn", low):
                level = "B2"
            elif re.search(r"pasivn", low):
                level = "B1"
            elif re.search(r"z√°klad|zaklad|beginner|basic", low):
                level = "A1/A2"

        name = language_names.get(lang_code, lang_code.upper())
        if level:
            return f"{name} ({level})"
        return name

    def add_skill(t):
        t = clean_token(t)
        if is_noise(t):
            return
        low = t.lower()
        # Language skills with levels
        lang_skill = parse_language_skill(t)
        if lang_skill:
            if lang_skill not in seen_hard:
                seen_hard.add(lang_skill)
                hard_skills.append(lang_skill)
            return
        mapped = canonical_map.get(low, t)
        if mapped == "HTML, CSS":
            for part in ["HTML", "CSS"]:
                if part not in seen_hard:
                    seen_hard.add(part)
                    hard_skills.append(part)
            return
        if any(k in low for k in soft_keywords):
            if mapped not in seen_soft:
                seen_soft.add(mapped)
                soft_skills.append(mapped)
        else:
            if mapped not in seen_hard:
                seen_hard.add(mapped)
                hard_skills.append(mapped)

    for skill in skills:
        if not skill:
            continue
        # split by common separators
        parts = [skill]
        if any(sep in skill for sep in [",", ";", " | ", "/", " / "]):
            if "http" not in skill.lower():
                tmp = skill.replace(" / ", "/")
                parts = re.split(r"[;,]|\s\|\s|/", tmp)
        for p in parts:
            add_skill(p)

    return hard_skills, soft_skills


def extract_salary_range(stxt):
    """
    Centralizovan√° funkce pro extrakci platu z textu.
    Podporuje form√°ty: '35 000', '35.000', '35,000', '35-45 tis.' atd.
    """
    if not stxt:
        return None, None
    
    # Krok 1: Extrakce ƒç√≠seln√Ωch segment≈Ø vƒçetnƒõ oddƒõlovaƒç≈Ø (teƒçka, ƒç√°rka, mezera)
    nums = re.findall(r"\d[\d\s\.,]*", stxt)
    vals = []
    
    for x in nums:
        # Odstranƒõn√≠ mezer a teƒçek (ƒçasto tis√≠cov√© oddƒõlovaƒçe v ƒåR)
        cleaned = x.replace(" ", "").replace("\u00a0", "").replace(".", "")
        
        # O≈°et≈ôen√≠ ƒç√°rky - pokud je za n√≠ p≈ôesnƒõ 2 cifry na konci segmentu, jde o hal√©≈ôe
        if "," in cleaned:
            parts = cleaned.split(",")
            if len(parts) > 1 and len(parts[-1]) == 2:
                cleaned = parts[0] # Zahod√≠me hal√©≈ôe
            else:
                cleaned = cleaned.replace(",", "") # Jinak ƒç√°rka jako oddƒõlovaƒç tis√≠c≈Ø
        
        if cleaned:
            try:
                val = int(cleaned)
                # Filtrujeme nesmysln√© hodnoty (nap≈ô. '2024' z data nebo '1' z '1. patro')
                if val > 100:
                    vals.append(val)
            except ValueError:
                continue

    # Krok 2: Multiplik√°tor "tis√≠c"
    low_txt = stxt.lower()
    if "tis" in low_txt or "tis√≠c" in low_txt:
        # Pokud jsou hodnoty podez≈ôele mal√© (nap≈ô. 35 m√≠sto 35000), vyn√°sob√≠me je
        vals = [v * 1000 if v < 1000 else v for v in vals]

    # Krok 3: Urƒçen√≠ From/To
    salary_from = None
    salary_to = None
    
    if len(vals) == 1:
        salary_from = vals[0]
    elif len(vals) >= 2:
        # Se≈ôad√≠me, aby from < to (pokud by byly v opaƒçn√©m po≈ôad√≠ v textu)
        salary_from = min(vals[0], vals[1])
        salary_to = max(vals[0], vals[1])
        
    return salary_from, salary_to


# --- Ulo≈æen√≠ do Supabase ---
def save_job_to_supabase(job_data):
    if not supabase:
        print("Chyba: Supabase klient nen√≠ inicializov√°n, data nebudou ulo≈æena.")
        return False

    try:
        response = (
            supabase.table("jobs").select("url").eq("url", job_data["url"]).execute()
        )
        if response.data:
            print(f"    --> Nab√≠dka s URL {job_data['url']} ji≈æ existuje, p≈ôeskoƒçeno.")
            return True
    except Exception as e:
        print(f"Chyba p≈ôi kontrole duplicity: {e}")

    parsed_url = urlparse(job_data["url"])
    job_data["source"] = parsed_url.netloc.replace("www.", "")
    job_data.setdefault("scraped_at", now_iso())
    job_data.setdefault("legality_status", "legal") # Default to legal for scraped jobs
    
    # DETECT AND ASSIGN COUNTRY CODE based on domain
    domain = parsed_url.netloc.lower()
    if '.cz' in domain:
        job_data["country_code"] = "cs"
    elif '.sk' in domain:
        job_data["country_code"] = "sk"
    elif '.pl' in domain:
        job_data["country_code"] = "pl"
    elif '.de' in domain:
        job_data["country_code"] = "de"
    else:
        # Default to Czech if domain is unknown
        job_data["country_code"] = "cs"
    
    print(f"    üåç Country code: {job_data['country_code']} (detected from {domain})")

    if "language_code" not in job_data:
        lang_text = f"{job_data.get('title', '')} {job_data.get('description', '')}"
        detected_lang = detect_language_code(lang_text)
        if detected_lang:
            job_data["language_code"] = detected_lang
            print(f"    üàØ Detected language: {detected_lang}")
    
    # GEOCODE LOCATION: Convert location string to lat/lon
    if "location" in job_data and job_data["location"]:
        location_str = job_data["location"]
        print(f"    üåç Geocodov√°n√≠ lokality: {location_str}")
        
        geo_result = geocode_location(location_str)
        if geo_result:
            job_data["lat"] = geo_result["lat"]
            job_data["lng"] = geo_result["lon"]
            print(f"       ‚úÖ Nalezeno: ({geo_result['lat']:.4f}, {geo_result['lon']:.4f}) [{geo_result['source']}]")
        else:
            print(f"       ‚ö†Ô∏è Geolokace selhala, ulo≈æ sem bez sou≈ôadnic")
            job_data["lat"] = None
            job_data["lng"] = None

    try:
        response = supabase.table("jobs").insert(job_data).execute()
        if response.data:
            print(f"    --> Data pro '{job_data.get('title')}' √∫spƒõ≈°nƒõ ulo≈æena.")
            return True
        else:
            print(f"    ‚ùå Chyba p≈ôi ukl√°d√°n√≠ dat: {job_data.get('title')}")
            return False
    except Exception as e:
        print(f"    ‚ùå Do≈°lo k neoƒçek√°van√© chybƒõ p≈ôi ukl√°d√°n√≠: {e}")
        return False


# --- Stahov√°n√≠ str√°nky ---
def scrape_page(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        resp = requests.get(url, headers=headers, timeout=12)
        resp.raise_for_status()
        return BeautifulSoup(resp.content, "html.parser")
    except Exception as e:
        print(f"‚ùå Chyba p≈ôi stahov√°n√≠ {url}: {e}")
        return None


# --- Filtrov√°n√≠ footeru ---
def filter_out_junk(text):
    """Odstran√≠ navigaci, patiƒçky a obecn√Ω balast z popis≈Ø pozic."""
    if not text:
        return ""
    
    # Rozs√°hl√Ω seznam "junk" token≈Ø, kter√© se ƒçasto objevuj√≠ v navigaci nebo patiƒçk√°ch
    junk_tokens = [
        "nab√≠dky pr√°ce", "vytvo≈ôit si ≈æivotopis", "jobs.cz", "prace.cz", "atmoskop",
        "profesia.sk", "profesia.cz", "pr√°ca za rohom", "pr√°ce za rohem", "nelisa.com",
        "arnold", "teamio", "seduo.cz", "seduo.sk", "platy.cz", "platy.sk", "paylab.com",
        "mojposao", "historie odpovƒõd√≠", "ulo≈æen√© nab√≠dky", "upozornƒõn√≠ na nab√≠dky",
        "hled√°m zamƒõstnance", "vlo≈æit brig√°du", "cen√≠k inzerce", "napi≈°te n√°m",
        "pro m√©dia", "z√°sady ochrany soukrom√≠", "podm√≠nky pou≈æ√≠v√°n√≠", "nastaven√≠ cookies",
        "reklama na port√°lech", "transparentnost", "nahl√°sit nez√°konn√Ω obsah",
        "vzdƒõl√°vac√≠ kurzy", "st≈ôedo≈°kolsk√© nebo odborn√©", "typ pracovn√≠ho pomƒõru",
        "kontaktn√≠ √∫daje", "zadavatel", "ƒçast√© pracovn√≠ cesty", "foto v medailonku",
        "the pulse of beauty", "nadn√°rodn√≠ struktury", "vlastn√≠ organizace",
        "vyhrazen√Ω ƒças na inovace", "kafet√©rie", "p≈ô√≠spƒõvek na vzdƒõl√°n√≠",
        "stravenky/p≈ô√≠spƒõvek na stravov√°n√≠", "zdravotn√≠ volno/sickdays",
        "mo≈ænost obƒçasn√© pr√°ce z domova", "obƒçerstven√≠ na pracovi≈°ti",
        "p≈ô√≠spƒõvek na sport/kulturu", "firemn√≠ akce", "bonusy/pr√©mie",
        "flexibiln√≠ zaƒç√°tek/konec pracovn√≠ doby", "notebook", "sleva na firemn√≠ v√Ωrobky",
        "nab√≠dky pr√°ce", "brig√°dy", "inspirace", "zamƒõstnavatel√©", "skvƒõl√Ω ≈æivotopis",
        "m≈Ø≈æete si ho ulo≈æit", "vytisknout nebo poslat do svƒõta"
    ]
    
    lines = text.split("\n")
    filtered_lines = []
    
    for line in lines:
        stripped = line.strip()
        if not stripped:
            filtered_lines.append("")
            continue
            
        low = stripped.lower()
        
        # Pokud je ≈ô√°dek p≈ô√≠li≈° kr√°tk√Ω (navigaƒçn√≠ odkaz) a obsahuje junk token
        if len(stripped) < 100:
            if any(tok in low for tok in junk_tokens):
                continue
        
        # Specifick√© pro Jobs.cz navigaci (ƒçasto dlouh√© seznamy s kr√°tk√Ωmi ≈ô√°dky)
        if any(tok == low for tok in junk_tokens):
            continue
            
        filtered_lines.append(stripped)
    
    # Zpƒõtn√© spojen√≠ a vyƒçi≈°tƒõn√≠ pr√°zdn√Ωch ≈ô√°dk≈Ø na zaƒç√°tku/konci
    result = "\n".join(filtered_lines).strip()
    
    # Odstranƒõn√≠ v√≠cen√°sobn√Ωch pr√°zdn√Ωch ≈ô√°dk≈Ø
    result = re.sub(r'\n{3,}', '\n\n', result)
    
    return result if result else "Popis nen√≠ dostupn√Ω"


# Ponech√°v√°me filter_jenprace_footer pro zpƒõtnou kompatibilitu, 
# ale internƒõ vol√° filter_out_junk
def filter_jenprace_footer(text):
    return filter_out_junk(text)


# --- Jobs.cz ---
def scrape_jobs_cz(soup):
    jobs_saved = 0
    job_cards = soup.find_all("article")
    for card in job_cards:
        header = card.find("header")
        footer = card.find("footer")
        if not header or not footer:
            continue

        nazev_element = header.find("h2", class_="SearchResultCard__title")
        if not (nazev_element and nazev_element.a):
            continue
        title = norm_text(nazev_element.a.text)
        odkaz = urljoin("https://www.jobs.cz", nazev_element.a["href"])

        # Firma a lokalita
        company = "Nezn√°m√° spoleƒçnost"
        comp_el = footer.find("li", class_="SearchResultCard__footerItem")
        if comp_el:
            span = comp_el.find("span", {"translate": "no"})
            if span:
                company = norm_text(span.text)
        location = "Nezn√°m√° lokalita"
        loc_el = footer.find(
            "li",
            class_="SearchResultCard__footerItem",
            attrs={"data-test": "serp-locality"},
        )
        if loc_el:
            location = norm_text(loc_el.text)

        print(f"    Stahuji detail pro: {title}")
        detail_soup = scrape_page(odkaz)
        if not detail_soup:
            print(f"    --> Detail str√°nka nedostupn√°, p≈ôeskoƒçeno.")
            continue
        description = "Popis nenalezen"
        benefits = []
        salary_from = None
        salary_to = None
        salary_timeframe = None
        contract_type = "Nespecifikov√°no"
        employment_type = None
        job_level_notes = []

        if detail_soup:
            try:
                def extract_rich_text(container):
                    extracted = []
                    if not container:
                        return extracted
                    for elem in container.find_all(["p", "li", "h2", "h3", "h4", "ul", "ol"]):
                        if elem.name in ["ul", "ol"]:
                            continue
                        txt = norm_text(elem.get_text())
                        if not txt:
                            continue
                        if elem.name == "li":
                            extracted.append(f"- {txt}")
                        elif elem.name in ["h2", "h3", "h4"]:
                            extracted.append(f"\n### {txt}")
                        else:
                            extracted.append(txt)
                    return extracted

                parts = []
                header_intro = detail_soup.find("div", attrs={"data-test": "jd-header-text"})
                body_content = detail_soup.find("div", attrs={"data-jobad": "body"}) or detail_soup.find(
                    "div", attrs={"data-test": "jd-body-richtext"}
                )
                if not body_content:
                    body_content = detail_soup.find("div", class_="RichContent")
                if not body_content:
                    body_content = detail_soup.find("div", class_="JobDescriptionSection") or detail_soup.find(
                        "div", class_="JobDescription"
                    )

                parts.extend(extract_rich_text(header_intro))
                parts.extend(extract_rich_text(body_content))

                if parts:
                    description = filter_out_junk("\n\n".join(parts))


                # Benefity
                benefit_items = detail_soup.select("div.JobDescriptionBenefits [data-test='jd-benefits']")
                if benefit_items:
                    benefits = [norm_text(b.get_text()) for b in benefit_items if norm_text(b.get_text())]
                else:
                    btitle = detail_soup.find(
                        "p",
                        class_="typography-body-medium-text-regular JobDescriptionBenefits__title mb-600 text-secondary",
                    )
                    if btitle:
                        bdivs = btitle.find_next_siblings("div", class_="IconWithText")
                        benefits = [
                            norm_text(d.get_text())
                            for d in bdivs
                            if norm_text(d.get_text())
                        ]

                # Plat
                sal_div = detail_soup.find("div", {"data-test": "jd-salary"})
                if sal_div:
                    p = sal_div.find("p")
                    if p:
                        stxt = p.get_text(" ", strip=True)
                        salary_from, salary_to = extract_salary_range(stxt)
                        salary_timeframe = detect_salary_timeframe_cz(stxt)

                # Lokalita
                loc_link = detail_soup.find("a", {"data-test": "jd-info-location"})
                if loc_link:
                    location = norm_text(loc_link.get_text())

                # Typ smluvn√≠ho vztahu / pomƒõru + firma
                info_items = detail_soup.find_all("div", {"data-test": "jd-info-item"})
                for item in info_items:
                    label = item.find("span", class_="accessibility-hidden")
                    val = item.find("p")
                    if not (label and val):
                        continue
                    ltxt = label.get_text()
                    vtxt = norm_text(val.get_text())
                    if "Typ smluvn√≠ho vztahu" in ltxt:
                        contract_type = vtxt
                    elif "Typ pracovn√≠ho pomƒõru" in ltxt:
                        employment_type = vtxt
                    elif "Spoleƒçnost" in ltxt and vtxt:
                        company = vtxt
                    elif "Info" in ltxt and vtxt:
                        job_level_notes.append(vtxt)

                if not salary_timeframe and description:
                    salary_timeframe = detect_salary_timeframe_cz(description)
            except Exception as e:
                print(f"    ‚ùå Chyba detailu {odkaz}: {e}")

        if not benefits:
            benefits = ["Benefity nespecifikov√°ny"]

        working_time = detect_working_time_cz(employment_type or contract_type)
        work_model = detect_work_model_cz(location, title, description)
        job_level = detect_job_level_cz(f"{title} {description} {' '.join(job_level_notes)}")
        required_skills_raw = extract_required_skills_cz(detail_soup, description)
        hard_skills, soft_skills = normalize_required_skills_cz(required_skills_raw)
        required_skills = hard_skills + soft_skills

        job_data = {
            "title": title,
            "url": odkaz,
            "company": company,
            "location": location,
            "description": description,
            "benefits": benefits,
            "contract_type": contract_type,
            "salary_from": salary_from,
            "salary_to": salary_to,
            "salary_min": salary_from,
            "salary_max": salary_to,
            "salary_timeframe": salary_timeframe,
            "working_time": working_time,
            "work_model": work_model,
            "job_level": job_level,
            "required_skills": required_skills if required_skills else [],
            "salary_currency": "CZK",
        }

        # Skip external listings with missing/short content
        if is_external_listing(detail_soup, {"jobs.cz"}) and (not description or len(description) < 400):
            print(f"    ‚ö†Ô∏è Extern√≠ inzer√°t bez popisu, p≈ôeskakuji: {title}")
            continue

        # Quality Check
        if is_low_quality(job_data):
            print(f"    ‚ö†Ô∏è N√≠zk√° kvalita, p≈ôeskakuji: {title}")
            continue

        if save_job_to_supabase(job_data):
            jobs_saved += 1
        time.sleep(0.3)
    return jobs_saved


# --- Prace.cz ---
def scrape_prace_cz(soup):
    jobs_saved = 0
    job_links = soup.find_all("a", attrs={"data-jd": True})
    for link in job_links:
        title = norm_text(link.get_text())
        odkaz = urljoin("https://www.prace.cz", link["href"])
        print(f"    Stahuji detail pro: {title}")
        detail_soup = scrape_page(odkaz)
        if not detail_soup:
            print(f"    --> Detail str√°nka nedostupn√°, p≈ôeskoƒçeno.")
            continue

        company = "Nezn√°m√° spoleƒçnost"
        location = "Nezn√°m√° lokalita"
        employment_type = "Nespecifikov√°no"
        contract_type = "Nespecifikov√°no"
        benefits = []
        description = "Popis nenalezen"
        salary_from = None
        salary_to = None
        salary_timeframe = None

        if detail_soup:
            try:
                # Firma
                comp_el = detail_soup.find("dd", class_="advert__list--company-name")
                if comp_el:
                    strong = comp_el.find("strong")
                    if strong:
                        company = norm_text(strong.get_text())

                # Lokalita
                loc_el = detail_soup.find("dd", class_="advert__list--location")
                if loc_el:
                    span = loc_el.find("span", class_="data")
                    if span:
                        location = norm_text(span.get_text())

                # Druh √∫vazku
                empl_el = detail_soup.find("dd", class_="advert__list--employment-type")
                if empl_el:
                    div = empl_el.find("div", class_="data")
                    if div:
                        employment_type = norm_text(div.get_text())

                # Smluvn√≠ vztah
                contract_el = detail_soup.find(
                    "dd", class_="advert__list--contract-type"
                )
                if contract_el:
                    div = contract_el.find("div", class_="data")
                    if div:
                        contract_type = norm_text(div.get_text())

                # Benefity
                benefit_el = detail_soup.find("dd", class_="advert__list--benefit")
                if benefit_el:
                    span = benefit_el.find("span", class_="data")
                    if span:
                        benefits = [
                            b.strip() for b in span.get_text().split(",") if b.strip()
                        ]

                # Plat
                sal_h3 = detail_soup.find("h3", class_="advert__salary")
                if sal_h3:
                    stxt = sal_h3.get_text(" ", strip=True)
                    salary_from, salary_to = extract_salary_range(stxt)
                    salary_timeframe = detect_salary_timeframe_cz(stxt)

                # Popis
                desc_el = detail_soup.find("div", class_="advert__richtext")
                if desc_el:
                    parts = []
                    for child in desc_el.find_all(["p", "li", "h2", "h3"], recursive=True):
                        txt = norm_text(child.get_text())
                        if not txt:
                            continue
                        if child.name == "li":
                            parts.append(f"- {txt}")
                        elif child.name in ["h2", "h3"]:
                            parts.append(f"### {txt}")
                        else:
                            parts.append(txt)
                    
                    if parts:
                        description = filter_out_junk("\n\n".join(parts))
            except Exception as e:
                print(f"    ‚ùå Chyba detailu {odkaz}: {e}")

        if not benefits:
            benefits = ["Benefity nespecifikov√°ny"]

        working_time = detect_working_time_cz(contract_type or employment_type)
        work_model = detect_work_model_cz(location, title, description)
        job_level = detect_job_level_cz(title)
        required_skills_raw = extract_required_skills_cz(detail_soup, description)
        hard_skills, soft_skills = normalize_required_skills_cz(required_skills_raw)
        required_skills = hard_skills + soft_skills

        job_data = {
            "title": title,
            "url": odkaz,
            "company": company,
            "location": location,
            "description": description,
            "benefits": benefits,
            "contract_type": contract_type,
            "work_type": employment_type,
            "salary_from": salary_from,
            "salary_to": salary_to,
            "salary_min": salary_from,
            "salary_max": salary_to,
            "salary_timeframe": salary_timeframe,
            "working_time": working_time,
            "work_model": work_model,
            "job_level": job_level,
            "required_skills": required_skills if required_skills else [],
            "salary_currency": "CZK",
        }

        # Skip external listings with missing/short content
        if is_external_listing(detail_soup, {"prace.cz"}) and (not description or len(description) < 400):
            print(f"    ‚ö†Ô∏è Extern√≠ inzer√°t bez popisu, p≈ôeskakuji: {title}")
            continue

        if is_low_quality(job_data):
            print(f"    ‚ö†Ô∏è N√≠zk√° kvalita, p≈ôeskakuji: {title}")
            continue

        if save_job_to_supabase(job_data):
            jobs_saved += 1
        time.sleep(0.3)
    return jobs_saved


# --- Jenprace.cz --- (ponech√°no z p≈ôedchoz√≠ verze s benefity)
def scrape_jenprace_cz(soup):
    jobs_saved = 0
    job_cards = soup.find_all("a", class_="container-link")
    for card in job_cards:
        odkaz = card.get("href")
        if not odkaz or odkaz == "#":
            continue
        title_element = card.find("span", class_="offer-link")
        title = norm_text(title_element.text if title_element else "Nezn√°m√Ω n√°zev")
        detail_url = urljoin("https://www.jenprace.cz", odkaz)
        detail_soup = scrape_page(detail_url)
        if not detail_soup:
            print(f"    --> Detail str√°nka nedostupn√°, p≈ôeskoƒçeno.")
            continue

        # Oprava: hledej lokaci v detail_soup m√≠sto soup + pou≈æij fix_duplicated_city
        location = None
        if detail_soup:
            location_tag = detail_soup.select_one(
                "div.value[data-cy='locality-detail-value']"
            )
            if location_tag:
                location_text = location_tag.get_text(strip=True)
                location = fix_duplicated_city(location_text.split("(")[0].strip())
            else:
                location_tag = detail_soup.select_one(
                    "span.locality[data-cy='offer-locality']"
                )
                if location_tag:
                    location_text = location_tag.get_text(strip=True)
                    location = fix_duplicated_city(location_text.split("(")[0].strip())

        company = "Nezn√°m√° spoleƒçnost"
        if detail_soup:
            comp = detail_soup.find("div", {"data-cy": "company-value"})
            if comp:
                a = comp.find("a")
                company = norm_text(a.get_text() if a else comp.get_text())

        description = "Popis nenalezen"
        contract_type = "Nespecifikov√°no"
        education_level = "Nespecifikov√°no"
        benefits = []
        salary_from = None
        salary_to = None
        salary_timeframe = None

        if detail_soup:
            # Popis
            desc = detail_soup.find("div", class_="offer-content")
            if desc:
                parts = []
                for child in desc.find_all(["p", "li"], recursive=True):
                    txt = norm_text(child.get_text())
                    if not txt:
                        continue
                    if child.name == "li":
                        parts.append(f"- {txt}")
                    else:
                        parts.append(txt)
                
                if parts:
                    description = filter_out_junk("\n\n".join(parts))
            # Benefity
            blist = detail_soup.find("ul", {"data-cy": "offer-benefit-list"})
            if blist:
                items = blist.find_all("li", {"data-cy": "offer-benefit-item"})
                benefits = [
                    norm_text(i.get_text()) for i in items if norm_text(i.get_text())
                ]
            if not benefits:
                benefits = ["Benefity nespecifikov√°ny"]
            # Contract
            ct = detail_soup.find("div", {"data-cy": "relation-value"})
            if ct:
                contract_type = norm_text(ct.get_text())
            # Education
            edu = detail_soup.find("div", {"data-cy": "education-value"})
            if edu:
                education_level = norm_text(edu.get_text())

            # Plat
            sal_div = detail_soup.find("div", class_="label-reward")
            if sal_div:
                stxt = sal_div.get_text(" ", strip=True)
                salary_from, salary_to = extract_salary_range(stxt)
                salary_timeframe = detect_salary_timeframe_cz(stxt)

        required_skills_raw = extract_required_skills_cz(detail_soup, description)
        hard_skills, soft_skills = normalize_required_skills_cz(required_skills_raw)
        required_skills = hard_skills + soft_skills

        job_data = {
            "title": title,
            "url": detail_url,
            "company": company,
            "location": location,
            "description": description,
            "benefits": benefits,
            "contract_type": contract_type,
            "education_level": education_level,
            "salary_from": salary_from,
            "salary_to": salary_to,
            "salary_min": salary_from,
            "salary_max": salary_to,
            "salary_timeframe": salary_timeframe,
            "working_time": detect_working_time_cz(contract_type),
            "work_model": detect_work_model_cz(location, title, description),
            "job_level": detect_job_level_cz(title),
            "required_skills": required_skills if required_skills else [],
            "salary_currency": "CZK",
        }
        if is_low_quality(job_data):
            print(f"    ‚ö†Ô∏è N√≠zk√° kvalita, p≈ôeskakuji: {title}")
            continue
        if save_job_to_supabase(job_data):
            jobs_saved += 1
        time.sleep(0.3)
    return jobs_saved


# --- Hlavn√≠ funkce ---
def scrape_website(site_name, base_url, max_pages=10):
    total_saved = 0
    scrapers = {
        "jobs.cz": scrape_jobs_cz,
        "prace.cz": scrape_prace_cz,
        "jenprace.cz": scrape_jenprace_cz,
    }
    scraper_func = scrapers.get(site_name)
    if not scraper_func:
        print(f"‚ùå Nepodporovan√Ω web: {site_name}")
        return 0
    for page_num in range(1, max_pages + 1):
        url = (
            f"{base_url}&page={page_num}"
            if site_name == "jobs.cz"
            else f"{base_url}?page={page_num}"
        )
        soup = scrape_page(url)
        if not soup:
            continue
        jobs = scraper_func(soup)
        total_saved += jobs
        if jobs == 0:
            break
        time.sleep(2)
    print(f"Scrapov√°n√≠ {site_name} dokonƒçeno. Celkem {total_saved}.")
    return total_saved


def fix_duplicated_city(text):
    """Oprav√≠ duplicitn√≠ mƒõsto (OlomoucOlomouc -> Olomouc) a odstran√≠ 'zobrazit na mapƒõ'"""
    # Odstranƒõn√≠ textu "zobrazit na mapƒõ" a podobn√Ωch
    text = text.replace("zobrazit na mapƒõ", "").replace("Zobrazit na mapƒõ", "").strip()

    # Oprava duplicitn√≠ho mƒõsta
    if len(text) > 1 and len(text) % 2 == 0:
        half = len(text) // 2
        if text[:half] == text[half:]:
            return text[:half]
    return text


def run_all_scrapers():
    if not supabase:
        print("‚ùå Supabase nen√≠ dostupn√©. Scrapov√°n√≠ zru≈°eno.")
        return 0

    websites = [
        {
            "name": "jobs.cz",
            "base_url": "https://www.jobs.cz/prace/?language-skill=cs",
            "max_pages": 15,
        },
        {
            "name": "prace.cz",
            "base_url": "https://www.prace.cz/nabidky",
            "max_pages": 15,
        },
        {
            "name": "jenprace.cz",
            "base_url": "https://www.jenprace.cz/nabidky",
            "max_pages": 15,
        },
    ]

    grand_total = 0
    print(f"üöÄ Spou≈°t√≠m hromadn√© scrapov√°n√≠: {now_iso()}")
    for site in websites:
        try:
            grand_total += scrape_website(
                site["name"], site["base_url"], site["max_pages"]
            )
        except Exception as e:
            print(f"‚ùå Chyba p≈ôi scrapov√°n√≠ {site['name']}: {e}")

    print(f"‚úÖ Scrapov√°n√≠ dokonƒçeno. Celkem ulo≈æeno {grand_total} nab√≠dek.")
    return grand_total


if __name__ == "__main__":
    run_all_scrapers()
